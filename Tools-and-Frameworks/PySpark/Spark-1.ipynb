{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "## Table of contents\n",
    "[Step-0](#step-0-)\n",
    "[Step-1](#step-1-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-0 :\n",
    "Now create the SparkContext,A SparkContext represents the connection to a Spark cluster, and can be used to create an RDD and broadcast variables on that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/24 16:15:40 WARN Utils: Your hostname, dell-Precision-3260, resolves to a loopback address: 127.0.1.1; using 10.194.175.3 instead (on interface wlp0s20f3)\n",
      "25/07/24 16:15:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/24 16:15:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "print(pyspark.__version__)\n",
    "findspark.init()\n",
    "\n",
    "# Initialize Spark\n",
    "sc = SparkContext(\"local\",\"SparkLab\")\n",
    "sc.setCheckpointDir('checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-1 : \n",
    "Let's creating Resilient Distributed Datasets (RDDs). Pyspark has multiple methods to convert different datatypes into rdd. So, We first create our text file data.\n",
    "\n",
    "Sparkâ€™s primary abstraction is a distributed collection of items called a Resilient Distributed Dataset (RDD). RDDs can be created from Hadoop InputFormats (such as HDFS files) or by transforming other RDDs. \n",
    "\n",
    "1. **From Data files RDD creation**\n",
    "We will use textFile method from the SparkContext and method will read a text file from HDFS, a local file system(available on nodes) or any Hadoop-supported file system URI, and return it as an RDD of Strings.\n",
    "\n",
    "2. **Actions on RDD**\n",
    "RDDs have actions, which return values and we can easly perform the operations on rdd object such as counting the rows.\n",
    "\n",
    "3. Transformations, which return pointers to new RDDs.Thus, transformation won't display the output until the action is called ? We can use the transformation like filter transformation will return a new RDD with a subset of item in the file.\n",
    "\n",
    "\n",
    "\n",
    "Term                   |Definition\n",
    "----                   |-------\n",
    "RDD                    |Resilient Distributed Dataset\n",
    "Transformation         |Spark operation that produces an RDD\n",
    "Action                 |Spark operation that produces a local object\n",
    "Spark Job              |Sequence of transformations on data with a final action\n",
    "\n",
    "How to create the RDD ?\n",
    "There are two common ways to create the RDD\n",
    "\n",
    "Method                      |Result\n",
    "----------                               |-------\n",
    "`sc.parallelize(array)`                  |Create RDD of elements of array (or list)\n",
    "`sc.textFile(path/to/file)`                      |Create RDD of lines from file\n",
    "\n",
    "We can use transformations to create a set of instructions we want to preform on the RDD\n",
    "Transformation Example                          |Result\n",
    "----------                               |-------\n",
    "`filter(lambda x: x % 2 == 0)`           |Discard non-even elements\n",
    "`map(lambda x: x * 2)`                   |Multiply each RDD element by `2`\n",
    "`map(lambda x: x.split())`               |Split each string into words\n",
    "`flatMap(lambda x: x.split())`           |Split each string into words and flatten sequence\n",
    "`sample(withReplacement=True,0.25)`      |Create sample of 25% of elements with replacement\n",
    "`union(rdd)`                             |Append `rdd` to existing RDD\n",
    "`distinct()`                             |Remove duplicates in RDD\n",
    "`sortBy(lambda x: x, ascending=False)`   |Sort elements in descending order\n",
    "\n",
    "We perform the transformation on the old rdd object and it returned pointer to the new rdd object then we can execute them by calling an action to return the output of new rdd.\n",
    "\n",
    "Action                             |Result\n",
    "----------                             |-------\n",
    "`collect()`                            |Convert RDD to in-memory list \n",
    "`take(3)`                              |First 3 elements of RDD \n",
    "`top(3)`                               |Top 3 elements of RDD\n",
    "`takeSample(withReplacement=True,3)`   |Create sample of 3 elements with replacement\n",
    "`sum()`                                |Find element sum (assumes numeric elements)\n",
    "`mean()`                               |Find element mean (assumes numeric elements)\n",
    "`stdev()`                              |Find element deviation (assumes numeric elements)\n",
    "\n",
    "We create transformation using the filter() method, is a filter function will only return element that satisfy the condition.\n",
    "\n",
    "Let's see the line that contain the word *programming*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset/example.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/example.txt\n",
    "Apache Spark is an open-source, distributed computing system designed \n",
    "or processing large-scale data quickly and efficiently. \n",
    "It provides an intuitive programming model for working with structured and \n",
    "unstructured data, enabling users to perform transformations and actions on datasets using high-level APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/example.txt MapPartitionsRDD[18] at textFile at NativeMethodAccessorImpl.java:0\n",
      "4\n",
      "Apache Spark is an open-source, distributed computing system designed \n",
      "PythonRDD[21] at RDD at PythonRDD.scala:56\n",
      "['It provides an intuitive programming model for working with structured and ']\n",
      "1\n",
      "[['It', 'provides', 'an', 'intuitive', 'programming', 'model', 'for', 'working', 'with', 'structured', 'and']]\n"
     ]
    }
   ],
   "source": [
    "# load the txt file data and create rdd\n",
    "textFile = sc.textFile('dataset/example.txt')\n",
    "print(textFile)\n",
    "\n",
    "# Action like count, first etc on rdd object\n",
    "print(textFile.count())\n",
    "print(textFile.first())\n",
    "\n",
    "# apply the transformation of the rdd object\n",
    "secfind = textFile.filter(lambda line: 'programming' in line)\n",
    "print(secfind)\n",
    "\n",
    "secfind2 = secfind.map(lambda line:line.split()).collect()\n",
    "# performing action on the rdd object because transformation returns pointer to the new-rdd.\n",
    "print(secfind.collect())\n",
    "print(secfind.count())\n",
    "print(secfind2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda expression allows developers to create anonymous functions, means we can quickly create function without defining function property def.\n",
    "\n",
    "\n",
    "Function objects returned by running lambda expressions work exactly the same as those created and assigned by defs. There is key difference that makes lambda useful in specialized roles:\n",
    "\n",
    "The lambda's body is similar to what we would put in a def body's return statement. We simply type the result as an expression instead of explicitly returning it. Because it is limited to an expression, a lambda is less general that a def. We can only squeeze design, to limit program nesting. lambda is designed for coding simple functions, and def handles the larger tasks.\n",
    "\n",
    "Note : lambda's body is a single expression, not a block of statements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square function\n",
    "def square(num):\n",
    "    return num**2\n",
    "\n",
    "# 2. \n",
    "def square(num): return num**2\n",
    "\n",
    "#3. \n",
    "lambda num: num**2\n",
    "square = lambda num: num**2\n",
    "\n",
    "#\n",
    "even = lambda x: x%2==0\n",
    "\n",
    "# grad the first character of the string\n",
    "first = lambda x: x[0]\n",
    "\n",
    "# reverse a string\n",
    "rev = lambda x: x[::-1]\n",
    "\n",
    "# we can accept more than one function in a lambda expression\n",
    "addition = lambda x, y : x + y\n",
    "\n",
    "# lambda function better used in conjuction with the map(),\n",
    "# filter() and reduce() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between Map vs flatMap ?\n",
    "\n",
    "We know RDD now, How to aggregate the value with them, it's only possible if we understand the concepts of working with Key Value Pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "dataset/example.txt MapPartitionsRDD[35] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "# collecting every thing as a single flat map\n",
    "text2 = sc.textFile('dataset/example.txt')\n",
    "print(text2.count())\n",
    "text2.flatMap(lambda line: line.split()).collect()\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset/service-example.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/service-example.txt\n",
    "#EventId    Timestamp    Customer   State    ServiceID    Amount\n",
    "301         07/05/2023    105       FL       143          250.00\n",
    "305         07/10/2023    210       IL       119          199.99\n",
    "302         07/08/2023    307       TX       163          350.50\n",
    "307         07/12/2023    111       CA       155          425.00\n",
    "303         07/09/2023    208       NY       147          675.25\n",
    "309         07/13/2023    120       FL       128          120.00\n",
    "304         07/09/2023    219       WA       126          305.75\n",
    "308         07/13/2023    305       OR       149          880.90\n",
    "306         07/11/2023    410       NV       161          405.60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#EventId    Timestamp    Customer   State    ServiceID    Amount', '301         07/05/2023    105       FL       143          250.00']\n",
      "[['#EventId', 'Timestamp', 'Customer', 'State', 'ServiceID', 'Amount'], ['301', '07/05/2023', '105', 'FL', '143', '250.00'], ['305', '07/10/2023', '210', 'IL', '119', '199.99']]\n",
      "['EventId    Timestamp    Customer   State    ServiceID    Amount', '301         07/05/2023    105       FL       143          250.00', '305         07/10/2023    210       IL       119          199.99', '302         07/08/2023    307       TX       163          350.50', '307         07/12/2023    111       CA       155          425.00', '303         07/09/2023    208       NY       147          675.25', '309         07/13/2023    120       FL       128          120.00', '304         07/09/2023    219       WA       126          305.75', '308         07/13/2023    305       OR       149          880.90', '306         07/11/2023    410       NV       161          405.60']\n"
     ]
    }
   ],
   "source": [
    "serv = sc.textFile('dataset/service-example.txt')\n",
    "print(serv.take(2))\n",
    "\n",
    "map1 = serv.map(lambda x: x.split()).take(3)\n",
    "print(map1)\n",
    "\n",
    "# removing the first hashtag\n",
    "serv_new = serv.map(lambda x: x[1:] if x[0]=='#' else x).collect()\n",
    "print(serv_new)\n",
    "\n",
    "serv2 = serv.map(lambda x: x[1:] if x[0]=='#' else x).map(lambda x: x.split()).collect()\n",
    "print(serv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use methods that combine lambda expressions via use a ByKey argument. These ByKey methods will assume that data is in a Key,Value form.\n",
    "\n",
    "find out the total sales per state ?\n",
    "using the key value pair for the operation on the rdd object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EventId', 'Timestamp', 'Customer', 'State', 'ServiceID', 'Amount'],\n",
       " ['301', '07/05/2023', '105', 'FL', '143', '250.00'],\n",
       " ['305', '07/10/2023', '210', 'IL', '119', '199.99'],\n",
       " ['302', '07/08/2023', '307', 'TX', '163', '350.50'],\n",
       " ['307', '07/12/2023', '111', 'CA', '155', '425.00'],\n",
       " ['303', '07/09/2023', '208', 'NY', '147', '675.25'],\n",
       " ['309', '07/13/2023', '120', 'FL', '128', '120.00'],\n",
       " ['304', '07/09/2023', '219', 'WA', '126', '305.75'],\n",
       " ['308', '07/13/2023', '305', 'OR', '149', '880.90'],\n",
       " ['306', '07/11/2023', '410', 'NV', '161', '405.60']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the data\n",
    "serv_clean = serv.map(lambda x: x[1:] if  x[0] == '#' else x).map(lambda x: x.split())\n",
    "serv_clean1 = serv_clean.collect()\n",
    "serv_clean1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('State', 'Amount'),\n",
       " ('FL', '250.00'),\n",
       " ('IL', '199.99'),\n",
       " ('TX', '350.50'),\n",
       " ('CA', '425.00'),\n",
       " ('NY', '675.25'),\n",
       " ('FL', '120.00'),\n",
       " ('WA', '305.75'),\n",
       " ('OR', '880.90'),\n",
       " ('NV', '405.60')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serv_data = serv_clean.map(lambda lst: (lst[3], lst[-1])).collect()\n",
    "serv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('State', 'Amount'),\n",
       " ('FL', '250.00120.00'),\n",
       " ('IL', '199.99'),\n",
       " ('TX', '350.50'),\n",
       " ('CA', '425.00'),\n",
       " ('NY', '675.25'),\n",
       " ('WA', '305.75'),\n",
       " ('OR', '880.90'),\n",
       " ('NV', '405.60')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serv_clean.map(lambda lst: (lst[3],lst[-1]))\\\n",
    "         .reduceByKey(lambda amt1,amt2 : amt1+amt2)\\\n",
    "         .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('State', 'Amount'),\n",
       " ('FL', 370.0),\n",
       " ('IL', '199.99'),\n",
       " ('TX', '350.50'),\n",
       " ('CA', '425.00'),\n",
       " ('NY', '675.25'),\n",
       " ('WA', '305.75'),\n",
       " ('OR', '880.90'),\n",
       " ('NV', '405.60')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it forget that amount are stil string thus \n",
    "serv_clean.map(lambda lst: (lst[3],lst[-1]))\\\n",
    "         .reduceByKey(lambda amt1,amt2 : float(amt1)+float(amt2))\\\n",
    "         .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab state and amounts\n",
    "# Add them\n",
    "# Get rid of ('State','Amount')\n",
    "# Sort them by the amount value\n",
    "\n",
    "# there is the some error related to the task manager \n",
    "serv_clean3 = serv.map(lambda x: x[1:] if  x[0] == '#' else x).map(lambda x: x.split())\n",
    "serv_clean3.map(lambda lst: (lst[3],lst[-1]))\\\n",
    ".reduceByKey(lambda amt1,amt2 : float(amt1)+float(amt2))\\\n",
    ".filter(lambda x: not x[0]=='State')\\\n",
    ".sortBy(lambda stateAmount: stateAmount[1], ascending=False)\\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
