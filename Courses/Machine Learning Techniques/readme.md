# **Machine Learning Techniques**

**Objective**

1. Understand the theory and practice of **supervised** and **unsupervised learning**.
2. Implement algorithms using **scikit-learn**, **NumPy**, and **TensorFlow/PyTorch**.
3. Apply regularization, feature selection, and model selection to real-world datasets.
4. Evaluate and compare models using metrics, validation, and cross-validation.

---

## **Table of Contents**

---

* [ ] **W1 : Unsupervised Learning — Representation Learning (PCA)**

  * [ ] **L1:** Implement **Principal Component Analysis (PCA)** from scratch and compare with `sklearn.decomposition.PCA`.
  * [ ] **L2:** Visualize dimensionality reduction on datasets (Iris, MNIST subset).

---

* [ ] **W2 : Unsupervised Learning — Kernel PCA**

  * [ ] **L1:** Derive and implement **Kernel PCA** using Gaussian (RBF) kernel.
  * [ ] **L2:** Compare linear vs. kernel PCA representations on nonlinear manifolds.

---

* [ ] **W3 : Unsupervised Learning — Clustering (K-Means / Kernel K-Means)**

  * [ ] **L1:** Implement **K-Means** and **Kernel K-Means** algorithms.
  * [ ] **L2:** Apply to image color quantization and customer segmentation datasets.

---

* [ ] **W4 : Unsupervised Learning — Estimation (MLE, Bayesian Estimation, GMM, EM Algorithm)**

  * [ ] **L1:** Recap and code **Maximum Likelihood Estimation (MLE)** and **Bayesian estimation** for simple distributions.
  * [ ] **L2:** Implement **Gaussian Mixture Model (GMM)** using **Expectation–Maximization (EM)**.

---

* [ ] **W5 : Supervised Learning — Regression (Least Squares; Bayesian View)**

  * [ ] **L1:** Derive and code **Ordinary Least Squares (OLS)** regression.
  * [ ] **L2:** Visualize uncertainty in predictions using a **Bayesian regression** approach.

---

* [ ] **W6 : Supervised Learning — Regularized Regression (Ridge / LASSO)**

  * [ ] **L1:** Implement **Ridge** and **LASSO** regression.
  * [ ] **L2:** Study bias–variance tradeoff and regularization paths using synthetic datasets.

---

* [ ] **W7 : Supervised Learning — Classification (K-NN, Decision Tree)**

  * [ ] **L1:** Implement **K-Nearest Neighbors (K-NN)** classifier.
  * [ ] **L2:** Build and visualize **Decision Trees** for categorical and numerical data.

---

* [ ] **W8 : Supervised Learning — Generative Models (Naïve Bayes)**

  * [ ] **L1:** Derive likelihood functions for **Naïve Bayes (Gaussian, Multinomial)**.
  * [ ] **L2:** Implement spam detection using Naïve Bayes on text datasets.

---

* [ ] **W9 : Discriminative Models (Perceptron, Logistic Regression)**

  * [ ] **L1:** Implement the **Perceptron algorithm** and visualize decision boundaries.
  * [ ] **L2:** Compare with **Logistic Regression**, analyze convergence and loss functions.

---

* [ ] **W10 : Support Vector Machines (SVM)**

  * [ ] **L1:** Implement linear **SVM** using the **dual form** and **kernel trick**.
  * [ ] **L2:** Apply to binary image or text classification using `sklearn.svm.SVC`.

---

* [ ] **W11 : Ensemble Methods — Bagging and Boosting (AdaBoost)**

  * [ ] **L1:** Implement **Bagging** with Decision Trees.
  * [ ] **L2:** Build **AdaBoost** classifier from scratch and compare with `sklearn.ensemble.AdaBoostClassifier`.

---

* [ ] **W12 : Artificial Neural Networks — Multiclass Classification**

  * [ ] **L1:** Implement a **Multilayer Perceptron (MLP)** using PyTorch/TensorFlow.
  * [ ] **L2:** Apply to **MNIST** or **Fashion-MNIST** dataset and analyze confusion matrices.

---
