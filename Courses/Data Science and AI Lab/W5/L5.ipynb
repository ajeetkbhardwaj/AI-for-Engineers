{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d32cbf5",
   "metadata": {},
   "source": [
    "# W5L5 : HuggingFace - Datasets, Tokenizers and Transformers\n",
    "- with Quantized LoRA training Llamma 3.1 3B model for the task of mail article summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7530394b",
   "metadata": {},
   "source": [
    "## Step-0 : Developement Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install necessary libraries\n",
    "!pip install -U datasets huggingface_hub fsspec aiohttp aiofiles transformers evaluate accelerate peft --quet\n",
    "!pip install -U bitsandbytes --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf77cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the lib modules for the our tasks\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TrainingArguments, Trainer\n",
    "from peft import LoraConfig,PeftModel\n",
    "from accelerate import Accelerator\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca40116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HF_token from google secret\n",
    "from google.colab import userdata\n",
    "HF_TOKEN= userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c77c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to the huggingface hug using hf token \n",
    "from huggingface_hub import login\n",
    "login(token='HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e976d90f",
   "metadata": {},
   "source": [
    "## Step-1 : Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccfc8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the daily mail dataset from huggingface\n",
    "data_name = \"cnn_dailymail\"\n",
    "dataset = load_dataset(data_name, '3.0.0', split=\"train[:10000]+validation[:2000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd1126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see few example of the dataset\n",
    "example = dataset[0]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec64a315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use dataset example into our prompt of the format\n",
    "def prompt_format(article, summary):\n",
    "    prompt_template = (\n",
    "        \"<|begin_of_text|>\"\n",
    "        \"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "        \"You are an expert summarizer. Your task is to provide a concise and accurate summary of the following news article. \"\n",
    "        \"The summary should capture the main points and key facts from the article, and should be no longer than 150 words.\"\n",
    "        \"<|eot_id|>\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        \"Article: {article}\"\n",
    "        \"<|eot_id|>\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        \"{summary}<|eot_id|>\"\n",
    "    )\n",
    "    return prompt_template.format(article=article, summary=summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let see prompt format from dataset example\n",
    "prompt_format(example['article'], example['highlights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a405a2c",
   "metadata": {},
   "source": [
    "## Step-2 : Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a1de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, \n",
    "                                bnb_4bit_quant=\"nf4\", \n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16, \n",
    "                                bnb_4bit_use_double_quant=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca20e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenization(example):\n",
    "    formatted_text = prompt_format(example['article'], example['highlights'])\n",
    "    tokenized_text = tokenizer(formatted_text, truncation=True, max_length=8192)\n",
    "    return tokenized_text\n",
    "tokenized_dataset = dataset.map(Tokenization, batched=False, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f25bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_data = processed_dataset[\"train\"]\n",
    "test_data = processed_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc550bac",
   "metadata": {},
   "source": [
    "## Step-3 :  Transformers\n",
    "- LoRA Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            quantization_config=bnb_config, \n",
    "                                            device_map=\"auto\", \n",
    "                                            trust_remote_code=True)\n",
    "\n",
    "print(f\"Number of parameters: {model.num_parameters()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ea5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(r=32, lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "\n",
    "\n",
    "model.add_adapter(lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-summarization-ft\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    max_steps=2000,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9379002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=8192,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    "    formatting_func=lambda example: [prompt_format(example[\"article\"], example[\"highlights\"])])\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0852b204",
   "metadata": {},
   "source": [
    "## Step-4 : Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the adapter\n",
    "output_dir = \"./llama3-70b-summarization-ft/final_adapter\"\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09c7267",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_for_inference = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True)\n",
    "\n",
    "\n",
    "model_for_inference = PeftModel.from_pretrained(base_model_for_inference, output_dir)\n",
    "tokenizer_for_inference = AutoTokenizer.from_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
