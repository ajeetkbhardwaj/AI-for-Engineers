{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f01e25",
   "metadata": {},
   "source": [
    "# W5L1 : HF Ecosystems\n",
    "It provides tools for building modern ml models.\n",
    "1. The Hub : a central repos for 1Ks of pre-trained and finetuned models and datasets\n",
    "2. Datasets library : \n",
    "3. Tokenizers library :\n",
    "4. Transformers library : \n",
    "5. Third party supports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee645e7",
   "metadata": {},
   "source": [
    "## Step-0 : Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install datasets tokenizers transformers -q > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b323db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets -> a standardized and memory efficient way to work with data.\n",
    "# how to load any public dataset from the huggingface hub ?\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91dd2f7",
   "metadata": {},
   "source": [
    "## Step-1 : Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38616f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset from hf hub public dataset\n",
    "wiki_data = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "\n",
    "# wiki_data is a DatasetDict containing different data splits for\n",
    "# training, testing and validation\n",
    "# lets view our dataset structure\n",
    "print(wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d344ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets access the train data\n",
    "train_data = wiki_data['train']\n",
    "print(f\"\\nTraining split info: {train_data}\")\n",
    "\n",
    "# View the features (columns) of the dataset\n",
    "print(f\"\\nFeatures: {train_data.features}\")\n",
    "\n",
    "# View a single example from the training data\n",
    "print(\"\\nFirst example:\")\n",
    "print(wiki_data['train'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd0eb5",
   "metadata": {},
   "source": [
    "## Step-2 : Tokenizers\n",
    "A translator that converts texts into a sequence of numbers(Token IDs). \n",
    "Modern Tokenizer : \n",
    "1. Subward tokenization of text\n",
    "2. Mapping each tokens to a unique token ID\n",
    "\n",
    "Task-1 : How to train a tokenizer onto custom dataset ?\n",
    "1. Text corpus iterator\n",
    "A function that provide the text to the tokenizer batch by batch rather than entire dataset in one go.\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f47549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
