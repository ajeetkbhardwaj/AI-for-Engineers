{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103ea0ae",
   "metadata": {},
   "source": [
    "What is transformer ?\n",
    "Transformers is a encoder-decoder architecture with attention mechanism at its base. It takes input as a sequence of words in a source language Then returns output also as sequence of words but lenght need not be same.\n",
    "![txt](images/1transformer.png)\n",
    "\n",
    "What is tokenizer and where does it fit into the training pipeline of transformers ?\n",
    "Tokenization is the process of splitting input text into the tokens. Ex- White  space tokenization, we split the sentence into words. Then Each unique token is maps to a unique ID and each ID is associated with a unique embedding vector in the embedding space.\n",
    "\n",
    "Then Language Model take these embedding vectors as input and predicts token IDs. During inference time these token IDs are converted back to the tokens and then words.\n",
    "![alt text](images/2tokenizers.png)\n",
    "Hence Tokenizer contains two components \n",
    "1. Encoder, which converts inputs to the token(word) to token ID\n",
    "2. Decoder, which converts token predicted by LM to the words i.e reverse operation of the encoder.\n",
    "\n",
    "The size of vocabulary determines the size of embedding space Then how to build(learn) a vocabulary $V$ from a large corpus of text that contains trillions of tokens ?\n",
    "\n",
    "What could be the reasonable size of vocabulary, we needed to build ?\n",
    "1. Arbitary size : x\n",
    "2. Small as number of characters : x\n",
    "3. Subwords : yes - because it provides reasonable size vocabular that feasible to apply softmax for probability prediction.\n",
    "\n",
    "Quest-1 : What are the challenges in building vocabulary for the given large corpus of text ?\n",
    "![alt text](images/3challengestokenization.png)\n",
    "\n",
    "Note : White space tokenizer is called pre-tokenization, we split sentence into the unique words then add all of them into vocabulary. We can also add special tokens like <go>, <stop>, <mask>, <sep> and <cls> and others to the vocabulary depending upon the types of downstream tasks and architectures(GPT/BERT) choice.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead958c4",
   "metadata": {},
   "source": [
    "### 2. HF-Tokenizers\n",
    "Tokenization Algorithms\n",
    "What are the wishlist we have for our tokenizer algorithms ?\n",
    "1. Moderate size vocabulary\n",
    "2. Efficiently handle agnostic words during inference\n",
    "3. Be language agnostic\n",
    "What are the different tokenization algorithms we have ?\n",
    "![alt text](images/4tokenizercategories.png)\n",
    "\n",
    "Pre-processing Pipeline - HF tokenizers module provides class that encapsulate all of these components.\n",
    "![alt text](images/5image.png)\n",
    "\n",
    "We can customize each step of tokenizer pipeline like 1. Normalizer : Lowercase, StripAccents\n",
    "2. Pre-Tokenizer : Whitespace, RegX and BERTlike etc.\n",
    "3. Algorithm(Model) : BPE, WordPiece, etc.\n",
    "4. Post Processor : Insert model specific tokens.\n",
    "![alt text](images/image.png)\n",
    "![alt text](images/image-1.png)\n",
    "\n",
    "![alt text](images/image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dcae1b",
   "metadata": {},
   "source": [
    "We will use the `bookcorpus`dataset to train our tokenizer. Make sure that you have sufficient memory because it will take approx 5GP of memory space to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40594c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and Train a Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# tokenizer class build\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizer.models import BPE\n",
    "from tokenizer.trainer import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894bcd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('bookcorpus', split='all')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "for idx, sample in enumerate(ds[0:num_samples]['text'])\n",
    "   print(f\"{idx} : {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c471666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's the pipeline of tokenizer class\n",
    "# 1. Normalizer : Lowercase\n",
    "# 2. Pre-tokenizer : Whitespace\n",
    "# 3. Tokenizer model : BPE\n",
    "# 4. Post Processor : x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab35981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intiate the tokenizer model : BPE with special unknown token and model will use it during prediction.\n",
    "model = BPE(unk_token=\"[UNK]\")\n",
    "tokenizer=Tokenizer(model)\n",
    "\n",
    "# addition of normalizer and pretokenizer to the pipeline\n",
    "tokenizer.normalizer = Lowercase()\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "# creating the trainer for the BPE with vocab_size and special tokens\n",
    "trainer = BpeTrainer(vocab_size=12000, special_tokens=[\"[PAD]\", \"[UNK]\"], continuing_subword_prefix='##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a404bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch processing by yield\n",
    "def get_examples(batch_size=1000):\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        yield ds[i: i + batch_size['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "print(cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5809e743",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(get_examples(batch_size=1000), trainer=trainer, length=len(ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48fb2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model.save('model', prefix='hopper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113513e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 10 merges\n",
    "with open('model/hopper-merges.txt', 'r') as file:\n",
    "    row = 0\n",
    "    num_lines = 10\n",
    "    for line in file.readlines():\n",
    "        print(line)\n",
    "        row += 1\n",
    "        if row >= num_lines:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d2ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last 10 merge\n",
    "with open('model/hopper-merges.txt', 'r') as file:\n",
    "    row = 0\n",
    "    num_lines = 10\n",
    "    for line in reversed(file.readlines()):\n",
    "        print(line)\n",
    "        row += 1\n",
    "        if row >= num_lines:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400fd3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the vocabulary\n",
    "# number of merges\n",
    "with open('model/hopper-merges.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "print(f\"Number of merges: {len(lines)}\")\n",
    "\n",
    "print(f\"Vocab Size: {tokenizer.get_vocab_size()}\")\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "# sort vocab by token IDs\n",
    "vocab_sorted = sorted(vocab.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6f97a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
