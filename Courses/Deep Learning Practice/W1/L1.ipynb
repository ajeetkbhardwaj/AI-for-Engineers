{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset hf module\n",
    "import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = load_dataset(\"stanfordnlp/imdb\")\n",
    "print(imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69276f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the train dataset from the dataset dictionary split\n",
    "imdb_train_data = imdb_data[\"train\"]\n",
    "print(imdb_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some part of dataset split\n",
    "_ = imdb_data.pop('unsupervised')\n",
    "print(imdb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ebd6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the train/test split alone without entire dataset download\n",
    "imdb_train_data = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
    "print(imdb_train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83fa815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can further divide the train dataset into the train and validation similar to the scikit learn\n",
    "imdb_val_data = imdb_train_data.train_test(test_size=0.2)\n",
    "print(imdb_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d235b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is we have also some local dataset train.csv and test.csv in csv format then\n",
    "# data  {train : , test : }\n",
    "data_files = [\"data/train.csv\", \"data/test.csv\"]\n",
    "local_data = load_dataset(\"csv\", data_files=data_files)\n",
    "print(local_data)\n",
    "# they have different supported format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd60259",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab2aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can convert data set from any format to the pyarrow format(memory efficient) \n",
    "# for saving space onto the memory disc.\n",
    "train_data.save_to_disc('pyarrow_dataset/movie_review')\n",
    "\n",
    "# now load the dataset from the local disk\n",
    "load_train_data_from_disc = load_from_disk('pyarrow_dataset/movie_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c54a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to access the specific samples from the dataset ?\n",
    "# sample index number \n",
    "idx = 1000\n",
    "example = imdb_data[\"train\"][idx]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bff4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the list samples from dataset by passing list of ids\n",
    "idx = range(0, 100, 2)\n",
    "sample = imdb_data[\"train\"].select([idx])\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation dataset\n",
    "from datasets import get_dataset_config_names, get_dataset_split_names\n",
    "print(get_dataset_config_names(\"wmt\"))\n",
    "print(get_dataset_split_names(\"wmt/wmt14\", \"hi-en\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the hi-en translation dataset\n",
    "trans_data = load_dataset(path=\"wmt/wmt14\", name=\"hi-en\")\n",
    "print(trans_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e702bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading all dataset as single without any split then\n",
    "raw_data=load_dataset(path=\"wmt/wmt14\", name=\"hi-en\", split=\"train+test+validate\")\n",
    "print(raw_data)\n",
    "print(len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features of the datasets like translation, [\"text\", \"label\"] etc\n",
    "# it defines the internal structure of the dataset that defines the underlying format of the dataset\n",
    "features = trans_data[\"train\"].features()\n",
    "print(features)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
