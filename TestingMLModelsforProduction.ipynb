{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d999c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. All dependencies imported.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any, List, Optional, Callable, Tuple\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import asyncio\n",
    "import hashlib\n",
    "import json\n",
    "from collections import defaultdict, deque\n",
    "from dataclasses import dataclass\n",
    "import threading\n",
    "\n",
    "# Scientific & statistical\n",
    "from scipy import stats\n",
    "from scipy.stats import wasserstein_distance, ks_2samp, ttest_ind\n",
    "\n",
    "# ML & data science\n",
    "from sklearn.datasets import make_classification, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "\n",
    "# Graphing and analysis\n",
    "#import networkx as nx\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Setup complete. All dependencies imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fca7a2",
   "metadata": {},
   "source": [
    "### A/B Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d2a16aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== A/B Test Router Demo ===\n",
      "{'legacy': {'count': 145, 'avg_prediction': 0.5379310344827586, 'avg_latency_ms': 1.3351308888402478, 'std_latency_ms': 6.82747961941215}, 'candidate': {'count': 55, 'avg_prediction': 0.509090909090909, 'avg_latency_ms': 0.4689173264936967, 'std_latency_ms': 1.0437011969191194}}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class SimpleABTestRouter:\n",
    "    \"\"\"\n",
    "    Basic A/B test router that randomly splits traffic.\n",
    "    \"\"\"\n",
    "    def __init__(self, legacy_model, candidate_model, \n",
    "                 candidate_traffic_fraction=0.1):\n",
    "        self.legacy_model = legacy_model\n",
    "        self.candidate_model = candidate_model\n",
    "        self.traffic_fraction = candidate_traffic_fraction\n",
    "        \n",
    "        # Track metrics for both groups\n",
    "        self.metrics = {\n",
    "            'legacy': {\n",
    "                'count': 0,\n",
    "                'predictions': [],\n",
    "                'latencies': []\n",
    "            },\n",
    "            'candidate': {\n",
    "                'count': 0,\n",
    "                'predictions': [],\n",
    "                'latencies': []\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def route_request(self, input_data: np.ndarray, user_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Route request to either legacy or candidate model.\n",
    "        Randomly assign based on traffic fraction.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Reshape input if it's 1D (single sample)\n",
    "        if len(input_data.shape) == 1:\n",
    "            input_data = input_data.reshape(1, -1)\n",
    "        \n",
    "        # Random assignment\n",
    "        if random.random() < self.traffic_fraction:\n",
    "            prediction = self.candidate_model.predict(input_data)\n",
    "            model_used = 'candidate'\n",
    "        else:\n",
    "            prediction = self.legacy_model.predict(input_data)\n",
    "            model_used = 'legacy'\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Log metrics\n",
    "        self.metrics[model_used]['count'] += 1\n",
    "        self.metrics[model_used]['predictions'].append(prediction[0])  # assuming predict returns array\n",
    "        self.metrics[model_used]['latencies'].append(latency)\n",
    "        \n",
    "        return {\n",
    "            'model': model_used,\n",
    "            'prediction': prediction[0],  # extract scalar\n",
    "            'latency': latency\n",
    "        }\n",
    "\n",
    "    def get_summary_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return aggregated metrics for both groups.\"\"\"\n",
    "        summary = {}\n",
    "        for group in ['legacy', 'candidate']:\n",
    "            metrics = self.metrics[group]\n",
    "            summary[group] = {\n",
    "                'count': metrics['count'],\n",
    "                'avg_prediction': np.mean(metrics['predictions']) if metrics['predictions'] else 0,\n",
    "                'avg_latency_ms': np.mean(metrics['latencies']) * 1000 if metrics['latencies'] else 0,\n",
    "                'std_latency_ms': np.std(metrics['latencies']) * 1000 if metrics['latencies'] else 0\n",
    "            }\n",
    "        return summary\n",
    "\n",
    "# Demo usage\n",
    "print(\"\\n=== A/B Test Router Demo ===\")\n",
    "legacy = LogisticRegression(random_state=42)\n",
    "candidate = LogisticRegression(random_state=42, C=0.5)\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=20, \n",
    "                          n_informative=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "legacy.fit(X_train, y_train)\n",
    "candidate.fit(X_train, y_train)\n",
    "\n",
    "# Create router and simulate requests\n",
    "router = SimpleABTestRouter(legacy, candidate, candidate_traffic_fraction=0.3)\n",
    "\n",
    "for i in range(200):\n",
    "    # Reshape sample into 2D array\n",
    "    sample = X_test[i].reshape(1, -1)\n",
    "    result = router.route_request(sample, user_id=f\"user_{i}\")\n",
    "\n",
    "print(router.get_summary_stats())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559688e",
   "metadata": {},
   "source": [
    "The output of your A/B Test Router Demo shows the key summary statistics for the \"legacy\" and \"candidate\" models after routing 200 requests with a 30% candidate traffic fraction:\n",
    "\n",
    "- Legacy model received 145 requests, candidate model received 55.\n",
    "- Average prediction values are ~0.538 for legacy and 0.509 for candidate.\n",
    "- Average latency for legacy is about 1.34 ms with high variance (std ~6.83 ms).\n",
    "- Candidate model shows faster average response time (~0.47 ms) with less variance (std ~1.04 ms).\n",
    "\n",
    "***\n",
    "\n",
    "- **Traffic split:** The fractional count roughly matches 70/30 expected split.\n",
    "- **Prediction averages:** Both models are producing predictions around 0.5, indicating similar output distributions (likely class probabilities or regression predictions centered near 0.5).\n",
    "- **Latency:** The candidate model appears more efficient or less loaded, showing lower latency and less variability than the legacy, although legacy's higher variance suggests some outliers or delays in some requests.\n",
    "- **Performance insight:** This baseline summary indicates the candidate can handle about 1/3rd of traffic with lower response times, and predictions are roughly similar in average value. Further analysis (like accuracy, significance tests) would be needed to conclude if the candidate is preferable.\n",
    "\n",
    "***\n",
    "\n",
    "This demonstrates how simple A/B traffic routing with metrics collection can provide actionable operational insights into model behavior in production settings, guiding rollout decisions with real user traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45609fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== A/B Test Significance ===\n",
      "p_value: 6.937972761017257e-49\n",
      "significant: True\n",
      "effect_size_cohens_d: 0.9832633766182235\n",
      "mean_a: 0.8507763942328004\n",
      "mean_b: 0.8702101723594721\n",
      "ci_a: (0.8490006148183638, 0.852552173647237)\n",
      "ci_b: (0.8685100541438253, 0.8719102905751188)\n",
      "interpretation: Significant difference\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def ab_test_significance(group_a_metrics: List[float], \n",
    "                        group_b_metrics: List[float],\n",
    "                        alpha: float = 0.05) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform two‑sample t‑test to determine if groups differ significantly.\n",
    "    \n",
    "    Returns:\n",
    "        - p_value: probability of observing this difference by chance\n",
    "        - significant: whether difference is statistically significant\n",
    "        - effect_size: Cohen's d (magnitude of difference)\n",
    "        - confidence_interval: CI for mean difference\n",
    "    \"\"\"\n",
    "    # t‑test\n",
    "    t_stat, p_value = stats.ttest_ind(group_a_metrics, group_b_metrics)\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    mean_a, mean_b = np.mean(group_a_metrics), np.mean(group_b_metrics)\n",
    "    std_a, std_b = np.std(group_a_metrics), np.std(group_b_metrics)\n",
    "    n_a, n_b = len(group_a_metrics), len(group_b_metrics)\n",
    "    \n",
    "    pooled_std = np.sqrt(((n_a - 1) * std_a**2 + (n_b - 1) * std_b**2) / (n_a + n_b - 2))\n",
    "    cohens_d = (mean_b - mean_a) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    # Confidence intervals\n",
    "    sem_a = stats.sem(group_a_metrics)\n",
    "    sem_b = stats.sem(group_b_metrics)\n",
    "    ci_a = stats.t.interval(1 - alpha, n_a - 1, \n",
    "                           loc=mean_a, scale=sem_a)\n",
    "    ci_b = stats.t.interval(1 - alpha, n_b - 1, \n",
    "                           loc=mean_b, scale=sem_b)\n",
    "    \n",
    "    return {\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < alpha,\n",
    "        'effect_size_cohens_d': cohens_d,\n",
    "        'mean_a': mean_a,\n",
    "        'mean_b': mean_b,\n",
    "        'ci_a': ci_a,\n",
    "        'ci_b': ci_b,\n",
    "        'interpretation': 'Significant difference' if p_value < alpha \n",
    "                         else 'No significant difference'\n",
    "    }\n",
    "\n",
    "# Demo: Simulate A/B test results\n",
    "legacy_accuracy = np.random.normal(0.85, 0.02, 500)\n",
    "candidate_accuracy = np.random.normal(0.87, 0.02, 500)\n",
    "\n",
    "result = ab_test_significance(legacy_accuracy, candidate_accuracy)\n",
    "print(\"\\n=== A/B Test Significance ===\")\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3186d4d",
   "metadata": {},
   "source": [
    "The output from your \"=== A/B Test Significance ===\" shows results from a statistical test comparing two models:\n",
    "\n",
    "- **p_value ≈ 6.94e-49**: Extremely small p-value indicates the difference in performance between the two models is statistically significant.\n",
    "- **significant: True**: Confirms the difference is unlikely due to random chance.\n",
    "- **effect_size_cohens_d ≈ 0.98**: A large effect size, meaning the difference is not just statistically significant but also practically meaningful.\n",
    "- **mean_a ≈ 0.851 and mean_b ≈ 0.870**: The candidate model (b) has a higher mean accuracy than the legacy (a).\n",
    "- **Confidence intervals** for each mean are tight and do not overlap, reinforcing the significant difference.\n",
    "- **Interpretation: Significant difference**: The candidate model outperforms legacy with strong evidence.\n",
    "\n",
    "***\n",
    "\n",
    "Your A/B testing shows that the candidate model provides a **statistically and practically significant improvement** over the legacy model’s performance. This supports a strong case for adopting the candidate model in production, assuming other operational metrics (latency, stability) are acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee8a7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Canary Deployment Demo ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class CanaryRouter:\n",
    "    \"\"\"\n",
    "    Routes requests to candidate model for canary users,\n",
    "    legacy for rest. Deterministically assigns users based on hash.\n",
    "    \"\"\"\n",
    "    def __init__(self, legacy_model, candidate_model, \n",
    "                 canary_traffic_fraction=0.05):\n",
    "        self.legacy_model = legacy_model\n",
    "        self.candidate_model = candidate_model\n",
    "        self.canary_fraction = canary_traffic_fraction\n",
    "        \n",
    "        # Health tracking\n",
    "        self.canary_metrics = defaultdict(list)\n",
    "        self.legacy_metrics = defaultdict(list)\n",
    "        self.user_segments = set()\n",
    "        self.health_metrics = defaultdict(list)\n",
    "        self.is_healthy = True\n",
    "    \n",
    "    def is_canary_user(self, user_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Deterministically assign user to canary or not.\n",
    "        Same user always gets same assignment.\n",
    "        \"\"\"\n",
    "        hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)\n",
    "        return (hash_value % 100) < (self.canary_fraction * 100)\n",
    "    \n",
    "    def route_request(self, user_id: str, input_data: np.ndarray) -> Dict[str, Any]:\n",
    "        # Reshape if input_data is 1D (single sample)\n",
    "        if len(input_data.shape) == 1:\n",
    "            input_data = input_data.reshape(1, -1)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            if self.is_canary_user(user_id):\n",
    "                self.user_segments.add(user_id)\n",
    "                prediction = self.candidate_model.predict(input_data)\n",
    "                model_type = 'candidate'\n",
    "            else:\n",
    "                prediction = self.legacy_model.predict(input_data)\n",
    "                model_type = 'legacy'\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            self.health_metrics['latency'].append(latency)\n",
    "            return {'model': model_type, 'prediction': prediction[0], 'latency': latency}\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.health_metrics['error_rate'].append(1)\n",
    "            logger.error(f\"Error in route_request: {str(e)}\")\n",
    "            raise e\n",
    "    \n",
    "    def check_canary_health(self, \n",
    "                            latency_threshold_ms=100,\n",
    "                            error_threshold=0.01) -> bool:\n",
    "        \"\"\"\n",
    "        Check if canary metrics exceed thresholds.\n",
    "        Return False if unhealthy (should rollback).\n",
    "        \"\"\"\n",
    "        if not self.canary_metrics['latency']:\n",
    "            return True  # Not enough data yet\n",
    "        \n",
    "        avg_latency = np.mean(self.canary_metrics['latency'])\n",
    "        error_rate = len(self.canary_metrics['error']) / \\\n",
    "                     (len(self.canary_metrics['latency']) + \n",
    "                      len(self.canary_metrics['error']) + 1)\n",
    "        \n",
    "        is_healthy = (avg_latency * 1000 < latency_threshold_ms and\n",
    "                      error_rate < error_threshold)\n",
    "        \n",
    "        logger.info(f\"Canary health: latency={avg_latency*1000:.2f}ms, \"\n",
    "                    f\"error_rate={error_rate:.4f}, healthy={is_healthy}\")\n",
    "        \n",
    "        self.is_healthy = is_healthy\n",
    "        return is_healthy\n",
    "    \n",
    "    def expand_canary(self, new_fraction: float):\n",
    "        \"\"\"Increase canary traffic percentage.\"\"\"\n",
    "        old_fraction = self.canary_fraction\n",
    "        self.canary_fraction = min(new_fraction, 1.0)\n",
    "        logger.info(f\"Expanded canary from {old_fraction*100:.1f}% to \"\n",
    "                    f\"{self.canary_fraction*100:.1f}%\")\n",
    "\n",
    "# Demo\n",
    "print(\"\\n=== Canary Deployment Demo ===\")\n",
    "canary_router = CanaryRouter(legacy, candidate, canary_traffic_fraction=0.1)\n",
    "\n",
    "for i in range(100):\n",
    "    user_id = f\"user_{i}\"\n",
    "    result = canary_router.route_request(user_id, X_test[i])\n",
    "\n",
    "canary_router.check_canary_health()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee67192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Advanced Canary with Rollback ===\n",
      "Rollback triggered: False\n"
     ]
    }
   ],
   "source": [
    "class AdvancedCanaryRouter:\n",
    "    \"\"\"\n",
    "    Canary router with automatic rollback based on monitored metrics.\n",
    "    \"\"\"\n",
    "    def __init__(self, legacy_model, candidate_model,\n",
    "                 error_threshold=0.02,\n",
    "                 latency_threshold_ms=150,\n",
    "                 window_size=500):\n",
    "        self.legacy_model = legacy_model\n",
    "        self.candidate_model = candidate_model\n",
    "        self.error_threshold = error_threshold\n",
    "        self.latency_threshold_ms = latency_threshold_ms\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Sliding window of recent metrics\n",
    "        self.metrics_window = deque(maxlen=window_size)\n",
    "        self.canary_fraction = 0.05\n",
    "        self.is_healthy = True\n",
    "        self.rollback_triggered = False\n",
    "    \n",
    "    def route_request(self, user_id: str, input_data: Dict[str, Any]\n",
    "                     ) -> Dict[str, Any]:\n",
    "        \"\"\"Route request and monitor metrics.\"\"\"\n",
    "        start_time = time.time()\n",
    "        is_canary = (int(hashlib.md5(user_id.encode()).hexdigest(), 16) % 100 \n",
    "                    < self.canary_fraction * 100)\n",
    "        \n",
    "        try:\n",
    "            if is_canary and not self.rollback_triggered:\n",
    "                pred = self.candidate_model.predict(input_data)\n",
    "                model = 'candidate'\n",
    "            else:\n",
    "                pred = self.legacy_model.predict(input_data)\n",
    "                model = 'legacy'\n",
    "            \n",
    "            latency_ms = (time.time() - start_time) * 1000\n",
    "            \n",
    "            self.metrics_window.append({\n",
    "                'model': model,\n",
    "                'latency_ms': latency_ms,\n",
    "                'error': False,\n",
    "                'timestamp': time.time()\n",
    "            })\n",
    "            \n",
    "            # Check health after each request (batched in practice)\n",
    "            if len(self.metrics_window) % 50 == 0:\n",
    "                self._check_and_handle_health()\n",
    "            \n",
    "            return {'model': model, 'prediction': pred, 'latency_ms': latency_ms}\n",
    "        \n",
    "        except Exception as e:\n",
    "            latency_ms = (time.time() - start_time) * 1000\n",
    "            self.metrics_window.append({\n",
    "                'model': 'candidate' if is_canary else 'legacy',\n",
    "                'latency_ms': latency_ms,\n",
    "                'error': True,\n",
    "                'timestamp': time.time()\n",
    "            })\n",
    "            raise\n",
    "    \n",
    "    def _check_and_handle_health(self):\n",
    "        \"\"\"Check metrics and trigger rollback if needed.\"\"\"\n",
    "        if len(self.metrics_window) < 100:\n",
    "            return\n",
    "        \n",
    "        # Calculate metrics on candidate traffic only\n",
    "        candidate_requests = [m for m in self.metrics_window \n",
    "                            if m['model'] == 'candidate']\n",
    "        \n",
    "        if not candidate_requests:\n",
    "            return\n",
    "        \n",
    "        error_rate = sum(1 for m in candidate_requests if m['error']) / len(candidate_requests)\n",
    "        avg_latency = np.mean([m['latency_ms'] for m in candidate_requests])\n",
    "        \n",
    "        is_unhealthy = (error_rate > self.error_threshold or\n",
    "                       avg_latency > self.latency_threshold_ms)\n",
    "        \n",
    "        if is_unhealthy and not self.rollback_triggered:\n",
    "            logger.warning(f\"ROLLBACK TRIGGERED: error_rate={error_rate:.4f}, \"\n",
    "                          f\"avg_latency_ms={avg_latency:.2f}\")\n",
    "            self.rollback_triggered = True\n",
    "            self.is_healthy = False\n",
    "\n",
    "# Demo\n",
    "print(\"\\n=== Advanced Canary with Rollback ===\")\n",
    "advanced_canary = AdvancedCanaryRouter(legacy, candidate)\n",
    "\n",
    "for i in range(300):\n",
    "    user_id = f\"user_{i}\"\n",
    "    try:\n",
    "        result = advanced_canary.route_request(user_id, X_test[i % len(X_test)])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"Rollback triggered: {advanced_canary.rollback_triggered}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc5321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
